{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chemin nltk_data utilisé : c:\\Users\\ghali\\Documents\\projet_autocorrect\\nltk_data\n",
      "Liste des chemins recherchés par NLTK : ['c:\\\\Users\\\\ghali\\\\Documents\\\\projet_autocorrect\\\\nltk_data', 'C:\\\\Users\\\\ghali/nltk_data', 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\nltk_data', 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\share\\\\nltk_data', 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\ghali\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     c:\\Users\\ghali\\Documents\\projet_autocorrect\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     c:\\Users\\ghali\\Documents\\projet_autocorrect\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     c:\\Users\\ghali\\Documents\\projet_autocorrect\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "# Définir le chemin local pour nltk_data (dans ton dossier de projet)\n",
    "nltk_data_path = os.path.join(os.getcwd(), 'nltk_data')\n",
    "\n",
    "# Créer le dossier s'il n'existe pas\n",
    "if not os.path.exists(nltk_data_path):\n",
    "    os.makedirs(nltk_data_path)\n",
    "\n",
    "# Forcer NLTK à utiliser ce dossier en le plaçant en tête de la liste\n",
    "os.environ['NLTK_DATA'] = nltk_data_path\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.insert(0, nltk_data_path)\n",
    "\n",
    "# Télécharger les ressources nécessaires, y compris 'punkt_tab'\n",
    "nltk.download('punkt', download_dir=nltk_data_path)\n",
    "nltk.download('punkt_tab', download_dir=nltk_data_path)  # <-- Ajout pour résoudre le LookupError\n",
    "nltk.download('gutenberg', download_dir=nltk_data_path)\n",
    "\n",
    "print(\"Chemin nltk_data utilisé :\", nltk_data_path)\n",
    "print(\"Liste des chemins recherchés par NLTK :\", nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier english.pickle est trouvé !\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt/english.pickle')\n",
    "    print(\"Le fichier english.pickle est trouvé !\")\n",
    "except LookupError:\n",
    "    print(\"Le fichier english.pickle est introuvable. Réessaie le téléchargement.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de mots : 83034\n",
      "Nombre de mots uniques : 5653\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Charger le texte du livre \"Emma\" de Jane Austen\n",
    "texte = gutenberg.raw('austen-persuasion.txt')\n",
    "\n",
    "# Tokeniser le texte en le mettant en minuscule\n",
    "tokens = word_tokenize(texte.lower())\n",
    "\n",
    "# Garder uniquement les mots alphabétiques\n",
    "mots = [mot for mot in tokens if mot.isalpha()]\n",
    "\n",
    "# Créer un vocabulaire unique (ensemble des mots)\n",
    "vocabulaire = set(mots)\n",
    "\n",
    "print(\"Nombre total de mots :\", len(mots))\n",
    "print(\"Nombre de mots uniques :\", len(vocabulaire))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def generer_candidats(mot, vocabulaire, n=3):\n",
    "    \"\"\"\n",
    "    Retourne jusqu'à n candidats proches pour le mot donné.\n",
    "    \"\"\"\n",
    "    return difflib.get_close_matches(mot, vocabulaire, n=n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Calculer la fréquence de chaque mot dans le corpus\n",
    "frequences = Counter(mots)\n",
    "\n",
    "def meilleure_correction(mot, vocabulaire, frequences):\n",
    "    \"\"\"\n",
    "    Retourne la correction la plus probable pour un mot mal orthographié,\n",
    "    en choisissant parmi les candidats celui qui a la fréquence la plus élevée.\n",
    "    \"\"\"\n",
    "    candidats = generer_candidats(mot, vocabulaire)\n",
    "    if not candidats:\n",
    "        return mot  # Aucun candidat trouvé, on retourne le mot original\n",
    "    return max(candidats, key=lambda c: frequences[c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correction de 'definately' => 'infinitely'\n"
     ]
    }
   ],
   "source": [
    "mot_test = \"definately\"\n",
    "correction = meilleure_correction(mot_test, vocabulaire, frequences)\n",
    "print(f\"Correction de '{mot_test}' => '{correction}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant : I definately recieve teh best gifts\n",
      "Après : i infinitely receive the best its\n"
     ]
    }
   ],
   "source": [
    "def corriger_phrase(phrase, vocabulaire, frequences):\n",
    "    tokens = word_tokenize(phrase.lower())\n",
    "    # Pour chaque mot, si c'est alphabétique, on applique la correction\n",
    "    mots_corriges = [\n",
    "        meilleure_correction(mot, vocabulaire, frequences) if mot.isalpha() else mot\n",
    "        for mot in tokens\n",
    "    ]\n",
    "    return ' '.join(mots_corriges)\n",
    "\n",
    "# Exemple de test sur une phrase\n",
    "exemple = \"I definately recieve teh best gifts\"\n",
    "print(\"Avant :\", exemple)\n",
    "print(\"Après :\", corriger_phrase(exemple, vocabulaire, frequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "# Construire les bigrammes à partir de la liste \"mots\"\n",
    "bigram_counts = Counter(ngrams(mots, 2))\n",
    "unigram_counts = Counter(mots)\n",
    "\n",
    "def bigram_probability(prev, word):\n",
    "    \"\"\"\n",
    "    Calcule la probabilité P(word|prev) = count(prev, word) / count(prev)\n",
    "    \"\"\"\n",
    "    if unigram_counts[prev] == 0:\n",
    "        return 0\n",
    "    return bigram_counts[(prev, word)] / unigram_counts[prev]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meilleure_correction_context(mot, vocabulaire, frequences, prev_word=None):\n",
    "    \"\"\"\n",
    "    Correction d'un mot en tenant compte du contexte (mot précédent).\n",
    "    Si prev_word est fourni, on évalue chaque candidat avec :\n",
    "        score = bigram_probability(prev_word, candidat) * frequences[candidat]\n",
    "    Sinon, on utilise la fonction de correction de base.\n",
    "    \"\"\"\n",
    "    candidats = generer_candidats(mot, vocabulaire)\n",
    "    if not candidats:\n",
    "        return mot\n",
    "    if prev_word is None:\n",
    "        return max(candidats, key=lambda c: frequences[c])\n",
    "    \n",
    "    # Calculer le score pour chaque candidat\n",
    "    scores = {}\n",
    "    for candidat in candidats:\n",
    "        # On combine la probabilité bigramme avec la fréquence (pour différencier les candidats)\n",
    "        prob = bigram_probability(prev_word, candidat)\n",
    "        scores[candidat] = prob * frequences[candidat]\n",
    "    \n",
    "    # Si tous les scores sont nuls, on retombe sur la fréquence seule\n",
    "    if all(score == 0 for score in scores.values()):\n",
    "        return max(candidats, key=lambda c: frequences[c])\n",
    "    \n",
    "    return max(scores, key=scores.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant : I definately recieve teh best gifts\n",
      "Après : i infinitely receive the best its\n"
     ]
    }
   ],
   "source": [
    "def corriger_phrase_context(phrase, vocabulaire, frequences):\n",
    "    tokens = word_tokenize(phrase.lower())\n",
    "    mots_corriges = []\n",
    "    prev_word = None\n",
    "    for mot in tokens:\n",
    "        if mot.isalpha():\n",
    "            # Utilise la correction contextuelle\n",
    "            correction = meilleure_correction_context(mot, vocabulaire, frequences, prev_word)\n",
    "            mots_corriges.append(correction)\n",
    "            prev_word = correction  # On utilise le mot corrigé pour le contexte suivant\n",
    "        else:\n",
    "            mots_corriges.append(mot)\n",
    "            prev_word = None  # Reset si on a de la ponctuation\n",
    "    return ' '.join(mots_corriges)\n",
    "\n",
    "# Test de la fonction avec contexte\n",
    "exemple_context = \"I definately recieve teh best gifts\"\n",
    "print(\"Avant :\", exemple_context)\n",
    "print(\"Après :\", corriger_phrase_context(exemple_context, vocabulaire, frequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(lang=\"en\"):\n",
    "    \"\"\"\n",
    "    Charge un corpus en fonction de la langue.\n",
    "    Pour \"en\", on utilise le corpus Gutenberg (austen-emma.txt).\n",
    "    Pour \"fr\", on charge le fichier \"corpus_fr.txt\" (à créer par l'utilisateur).\n",
    "    \"\"\"\n",
    "    if lang == \"en\":\n",
    "        from nltk.corpus import gutenberg\n",
    "        texte = gutenberg.raw('austen-emma.txt')\n",
    "    elif lang == \"fr\":\n",
    "        try:\n",
    "            with open(\"corpus_fr.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "                texte = f.read()\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\"Le fichier corpus_fr.txt n'a pas été trouvé dans le dossier du projet.\")\n",
    "    else:\n",
    "        raise ValueError(\"Langue non supportée. Choisir 'en' ou 'fr'.\")\n",
    "    return texte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocabulary(texte, language=\"en\"):\n",
    "    \"\"\"\n",
    "    Tokenise le texte en utilisant le paramètre de langue, filtre les tokens et crée le vocabulaire.\n",
    "    Retourne un ensemble de mots et un Counter des fréquences.\n",
    "    \"\"\"\n",
    "    # Utilise \"english\" si lang == \"en\", sinon \"french\"\n",
    "    lang_param = \"english\" if language==\"en\" else \"french\"\n",
    "    tokens = word_tokenize(texte.lower(), language=lang_param)\n",
    "    mots = [mot for mot in tokens if mot.isalpha()]\n",
    "    vocabulaire = set(mots)\n",
    "    frequences = Counter(mots)\n",
    "    print(f\"Corpus chargé en {language}: {len(mots)} mots, {len(vocabulaire)} uniques.\")\n",
    "    return vocabulaire, frequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def generer_candidats(mot, vocabulaire, n=3):\n",
    "    return difflib.get_close_matches(mot, vocabulaire, n=n)\n",
    "\n",
    "def meilleure_correction(mot, vocabulaire, frequences):\n",
    "    candidats = generer_candidats(mot, vocabulaire)\n",
    "    if not candidats:\n",
    "        return mot\n",
    "    return max(candidats, key=lambda c: frequences[c])\n",
    "\n",
    "def corriger_phrase(phrase, vocabulaire, frequences, lang=\"en\"):\n",
    "    tokens = word_tokenize(phrase.lower(), language=\"english\" if lang==\"en\" else \"french\")\n",
    "    mots_corriges = [\n",
    "        meilleure_correction(mot, vocabulaire, frequences) if mot.isalpha() else mot\n",
    "        for mot in tokens\n",
    "    ]\n",
    "    return ' '.join(mots_corriges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus chargé en en: 157110 mots, 6932 uniques.\n",
      "Corpus chargé en en: 157110 mots, 6932 uniques.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "\n",
    "# Variables globales pour le vocabulaire et les fréquences\n",
    "vocabulaire = None\n",
    "frequences = None\n",
    "\n",
    "# Fonction pour recharger le corpus en fonction de la langue choisie\n",
    "def reload_corpus():\n",
    "    lang = current_language.get()\n",
    "    try:\n",
    "        texte = load_corpus(lang)\n",
    "        global vocabulaire, frequences\n",
    "        vocabulaire, frequences = build_vocabulary(texte, language=lang)\n",
    "        output_label.config(text=f\"Corpus chargé en {lang.upper()}\")\n",
    "    except Exception as e:\n",
    "        output_label.config(text=str(e))\n",
    "\n",
    "# Fonction pour corriger le texte saisi dans l'interface\n",
    "def corriger_texte():\n",
    "    phrase = entry.get()\n",
    "    lang = current_language.get()\n",
    "    # Vérifier que le vocabulaire est bien chargé\n",
    "    if vocabulaire is None or frequences is None:\n",
    "        output_label.config(text=\"Veuillez d'abord charger le corpus en choisissant la langue.\")\n",
    "        return\n",
    "    phrase_corrigee = corriger_phrase(phrase, vocabulaire, frequences, lang)\n",
    "    output_label.config(text=phrase_corrigee)\n",
    "\n",
    "# Création de la fenêtre principale\n",
    "root = tk.Tk()\n",
    "root.title(\"Auto-correcteur NLP avec choix de langue\")\n",
    "\n",
    "# Menu déroulant pour choisir la langue (en ou fr)\n",
    "current_language = tk.StringVar(root)\n",
    "current_language.set(\"en\")  # Par défaut en anglais\n",
    "\n",
    "language_menu = tk.OptionMenu(root, current_language, \"en\", \"fr\")\n",
    "language_menu.config(font=(\"Arial\", 12))\n",
    "language_menu.pack(pady=5)\n",
    "\n",
    "# Bouton pour recharger le corpus selon la langue choisie\n",
    "reload_button = tk.Button(root, text=\"Charger corpus\", command=reload_corpus, font=(\"Arial\", 12))\n",
    "reload_button.pack(pady=5)\n",
    "\n",
    "# Zone de saisie du texte à corriger\n",
    "entry = tk.Entry(root, width=50, font=(\"Arial\", 12))\n",
    "entry.pack(pady=10)\n",
    "\n",
    "# Bouton pour lancer la correction\n",
    "bouton = tk.Button(root, text=\"Corriger\", command=corriger_texte, font=(\"Arial\", 12))\n",
    "bouton.pack(pady=5)\n",
    "\n",
    "# Zone d'affichage du résultat\n",
    "output_label = tk.Label(root, text=\"\", wraplength=400, font=(\"Arial\", 12))\n",
    "output_label.pack(pady=10)\n",
    "\n",
    "# Lancement de l'interface\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
