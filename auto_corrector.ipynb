{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chemin nltk_data utilisé : c:\\Users\\ghali\\Documents\\projet_autocorrect\\nltk_data\n",
      "Liste des chemins recherchés par NLTK : ['c:\\\\Users\\\\ghali\\\\Documents\\\\projet_autocorrect\\\\nltk_data', 'C:\\\\Users\\\\ghali/nltk_data', 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\nltk_data', 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\share\\\\nltk_data', 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\ghali\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     c:\\Users\\ghali\\Documents\\projet_autocorrect\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     c:\\Users\\ghali\\Documents\\projet_autocorrect\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     c:\\Users\\ghali\\Documents\\projet_autocorrect\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "# Définir le chemin local pour nltk_data (dans ton dossier de projet)\n",
    "nltk_data_path = os.path.join(os.getcwd(), 'nltk_data')\n",
    "\n",
    "# Créer le dossier s'il n'existe pas\n",
    "if not os.path.exists(nltk_data_path):\n",
    "    os.makedirs(nltk_data_path)\n",
    "\n",
    "# Forcer NLTK à utiliser ce dossier en le plaçant en tête de la liste\n",
    "os.environ['NLTK_DATA'] = nltk_data_path\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.insert(0, nltk_data_path)\n",
    "\n",
    "# Télécharger les ressources nécessaires, y compris 'punkt_tab'\n",
    "nltk.download('punkt', download_dir=nltk_data_path)\n",
    "nltk.download('punkt_tab', download_dir=nltk_data_path)  # <-- Ajout pour résoudre le LookupError\n",
    "nltk.download('gutenberg', download_dir=nltk_data_path)\n",
    "\n",
    "print(\"Chemin nltk_data utilisé :\", nltk_data_path)\n",
    "print(\"Liste des chemins recherchés par NLTK :\", nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier english.pickle est trouvé !\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt/english.pickle')\n",
    "    print(\"Le fichier english.pickle est trouvé !\")\n",
    "except LookupError:\n",
    "    print(\"Le fichier english.pickle est introuvable. Réessaie le téléchargement.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de mots : 83034\n",
      "Nombre de mots uniques : 5653\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Charger le texte du livre \"Emma\" de Jane Austen\n",
    "texte = gutenberg.raw('austen-persuasion.txt')\n",
    "\n",
    "# Tokeniser le texte en le mettant en minuscule\n",
    "tokens = word_tokenize(texte.lower())\n",
    "\n",
    "# Garder uniquement les mots alphabétiques\n",
    "mots = [mot for mot in tokens if mot.isalpha()]\n",
    "\n",
    "# Créer un vocabulaire unique (ensemble des mots)\n",
    "vocabulaire = set(mots)\n",
    "\n",
    "print(\"Nombre total de mots :\", len(mots))\n",
    "print(\"Nombre de mots uniques :\", len(vocabulaire))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def generer_candidats(mot, vocabulaire, n=3):\n",
    "    \"\"\"\n",
    "    Retourne jusqu'à n candidats proches pour le mot donné.\n",
    "    \"\"\"\n",
    "    return difflib.get_close_matches(mot, vocabulaire, n=n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Calculer la fréquence de chaque mot dans le corpus\n",
    "frequences = Counter(mots)\n",
    "\n",
    "def meilleure_correction(mot, vocabulaire, frequences):\n",
    "    \"\"\"\n",
    "    Retourne la correction la plus probable pour un mot mal orthographié,\n",
    "    en choisissant parmi les candidats celui qui a la fréquence la plus élevée.\n",
    "    \"\"\"\n",
    "    candidats = generer_candidats(mot, vocabulaire)\n",
    "    if not candidats:\n",
    "        return mot  # Aucun candidat trouvé, on retourne le mot original\n",
    "    return max(candidats, key=lambda c: frequences[c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correction de 'definately' => 'infinitely'\n"
     ]
    }
   ],
   "source": [
    "mot_test = \"definately\"\n",
    "correction = meilleure_correction(mot_test, vocabulaire, frequences)\n",
    "print(f\"Correction de '{mot_test}' => '{correction}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant : I definately recieve teh best gifts\n",
      "Après : i infinitely receive the best its\n"
     ]
    }
   ],
   "source": [
    "def corriger_phrase(phrase, vocabulaire, frequences):\n",
    "    tokens = word_tokenize(phrase.lower())\n",
    "    # Pour chaque mot, si c'est alphabétique, on applique la correction\n",
    "    mots_corriges = [\n",
    "        meilleure_correction(mot, vocabulaire, frequences) if mot.isalpha() else mot\n",
    "        for mot in tokens\n",
    "    ]\n",
    "    return ' '.join(mots_corriges)\n",
    "\n",
    "# Exemple de test sur une phrase\n",
    "exemple = \"I definately recieve teh best gifts\"\n",
    "print(\"Avant :\", exemple)\n",
    "print(\"Après :\", corriger_phrase(exemple, vocabulaire, frequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "# Construire les bigrammes à partir de la liste \"mots\"\n",
    "bigram_counts = Counter(ngrams(mots, 2))\n",
    "unigram_counts = Counter(mots)\n",
    "\n",
    "def bigram_probability(prev, word):\n",
    "    \"\"\"\n",
    "    Calcule la probabilité P(word|prev) = count(prev, word) / count(prev)\n",
    "    \"\"\"\n",
    "    if unigram_counts[prev] == 0:\n",
    "        return 0\n",
    "    return bigram_counts[(prev, word)] / unigram_counts[prev]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meilleure_correction_context(mot, vocabulaire, frequences, prev_word=None):\n",
    "    \"\"\"\n",
    "    Correction d'un mot en tenant compte du contexte (mot précédent).\n",
    "    Si prev_word est fourni, on évalue chaque candidat avec :\n",
    "        score = bigram_probability(prev_word, candidat) * frequences[candidat]\n",
    "    Sinon, on utilise la fonction de correction de base.\n",
    "    \"\"\"\n",
    "    candidats = generer_candidats(mot, vocabulaire)\n",
    "    if not candidats:\n",
    "        return mot\n",
    "    if prev_word is None:\n",
    "        return max(candidats, key=lambda c: frequences[c])\n",
    "    \n",
    "    # Calculer le score pour chaque candidat\n",
    "    scores = {}\n",
    "    for candidat in candidats:\n",
    "        # On combine la probabilité bigramme avec la fréquence (pour différencier les candidats)\n",
    "        prob = bigram_probability(prev_word, candidat)\n",
    "        scores[candidat] = prob * frequences[candidat]\n",
    "    \n",
    "    # Si tous les scores sont nuls, on retombe sur la fréquence seule\n",
    "    if all(score == 0 for score in scores.values()):\n",
    "        return max(candidats, key=lambda c: frequences[c])\n",
    "    \n",
    "    return max(scores, key=scores.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant : I definately recieve teh best gifts\n",
      "Après : i infinitely receive the best its\n"
     ]
    }
   ],
   "source": [
    "def corriger_phrase_context(phrase, vocabulaire, frequences):\n",
    "    tokens = word_tokenize(phrase.lower())\n",
    "    mots_corriges = []\n",
    "    prev_word = None\n",
    "    for mot in tokens:\n",
    "        if mot.isalpha():\n",
    "            # Utilise la correction contextuelle\n",
    "            correction = meilleure_correction_context(mot, vocabulaire, frequences, prev_word)\n",
    "            mots_corriges.append(correction)\n",
    "            prev_word = correction  # On utilise le mot corrigé pour le contexte suivant\n",
    "        else:\n",
    "            mots_corriges.append(mot)\n",
    "            prev_word = None  # Reset si on a de la ponctuation\n",
    "    return ' '.join(mots_corriges)\n",
    "\n",
    "# Test de la fonction avec contexte\n",
    "exemple_context = \"I definately recieve teh best gifts\"\n",
    "print(\"Avant :\", exemple_context)\n",
    "print(\"Après :\", corriger_phrase_context(exemple_context, vocabulaire, frequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(lang=\"en\"):\n",
    "    \"\"\"\n",
    "    Charge un corpus en fonction de la langue.\n",
    "    Pour \"en\", on utilise le corpus Gutenberg (austen-emma.txt).\n",
    "    Pour \"fr\", on charge le fichier \"corpus_fr.txt\" (à créer par l'utilisateur).\n",
    "    \"\"\"\n",
    "    if lang == \"en\":\n",
    "        from nltk.corpus import gutenberg\n",
    "        texte = gutenberg.raw('austen-emma.txt')\n",
    "    elif lang == \"fr\":\n",
    "        try:\n",
    "            with open(\"corpus_fr.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "                texte = f.read()\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\"Le fichier corpus_fr.txt n'a pas été trouvé dans le dossier du projet.\")\n",
    "    else:\n",
    "        raise ValueError(\"Langue non supportée. Choisir 'en' ou 'fr'.\")\n",
    "    return texte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocabulary(texte, language=\"en\"):\n",
    "    \"\"\"\n",
    "    Tokenise le texte en utilisant le paramètre de langue, filtre les tokens et crée le vocabulaire.\n",
    "    Retourne un ensemble de mots et un Counter des fréquences.\n",
    "    \"\"\"\n",
    "    # Utilise \"english\" si lang == \"en\", sinon \"french\"\n",
    "    lang_param = \"english\" if language==\"en\" else \"french\"\n",
    "    tokens = word_tokenize(texte.lower(), language=lang_param)\n",
    "    mots = [mot for mot in tokens if mot.isalpha()]\n",
    "    vocabulaire = set(mots)\n",
    "    frequences = Counter(mots)\n",
    "    print(f\"Corpus chargé en {language}: {len(mots)} mots, {len(vocabulaire)} uniques.\")\n",
    "    return vocabulaire, frequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def generer_candidats(mot, vocabulaire, n=3):\n",
    "    return difflib.get_close_matches(mot, vocabulaire, n=n)\n",
    "\n",
    "def meilleure_correction(mot, vocabulaire, frequences):\n",
    "    candidats = generer_candidats(mot, vocabulaire)\n",
    "    if not candidats:\n",
    "        return mot\n",
    "    return max(candidats, key=lambda c: frequences[c])\n",
    "\n",
    "def corriger_phrase(phrase, vocabulaire, frequences, lang=\"en\"):\n",
    "    tokens = word_tokenize(phrase.lower(), language=\"english\" if lang==\"en\" else \"french\")\n",
    "    mots_corriges = [\n",
    "        meilleure_correction(mot, vocabulaire, frequences) if mot.isalpha() else mot\n",
    "        for mot in tokens\n",
    "    ]\n",
    "    return ' '.join(mots_corriges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5 (run):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\ghali\\AppData\\Local\\Temp\\ipykernel_10836\\2050975476.py\", line 92, in run\n",
      "  File \"C:\\Users\\ghali\\AppData\\Local\\Temp\\ipykernel_10836\\2050975476.py\", line 34, in build_vocabulary\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 142, in word_tokenize\n",
      "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
      "                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 119, in sent_tokenize\n",
      "    tokenizer = _get_punkt_tokenizer(language)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 105, in _get_punkt_tokenizer\n",
      "    return PunktTokenizer(language)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py\", line 1744, in __init__\n",
      "    self.load_lang(lang)\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py\", line 1749, in load_lang\n",
      "    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\ghali/nltk_data'\n",
      "    - 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\ghali\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "Exception in thread Thread-6 (run):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\ghali\\AppData\\Local\\Temp\\ipykernel_10836\\2050975476.py\", line 92, in run\n",
      "  File \"C:\\Users\\ghali\\AppData\\Local\\Temp\\ipykernel_10836\\2050975476.py\", line 34, in build_vocabulary\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 142, in word_tokenize\n",
      "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
      "                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 119, in sent_tokenize\n",
      "    tokenizer = _get_punkt_tokenizer(language)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 105, in _get_punkt_tokenizer\n",
      "    return PunktTokenizer(language)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py\", line 1744, in __init__\n",
      "    self.load_lang(lang)\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py\", line 1749, in load_lang\n",
      "    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/french/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\ghali/nltk_data'\n",
      "    - 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\ghali\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "Exception in thread Thread-7 (run):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\ghali\\AppData\\Local\\Temp\\ipykernel_10836\\2050975476.py\", line 92, in run\n",
      "  File \"C:\\Users\\ghali\\AppData\\Local\\Temp\\ipykernel_10836\\2050975476.py\", line 34, in build_vocabulary\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 142, in word_tokenize\n",
      "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
      "                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 119, in sent_tokenize\n",
      "    tokenizer = _get_punkt_tokenizer(language)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 105, in _get_punkt_tokenizer\n",
      "    return PunktTokenizer(language)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py\", line 1744, in __init__\n",
      "    self.load_lang(lang)\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py\", line 1749, in load_lang\n",
      "    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\ghali/nltk_data'\n",
      "    - 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\ghali\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\tkinter\\__init__.py\", line 1968, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ghali\\AppData\\Local\\Temp\\ipykernel_10836\\2050975476.py\", line 103, in corriger_interactivement\n",
      "    tokens = word_tokenize(phrase.lower(), language=\"english\" if lang == \"en\" else \"french\")\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 142, in word_tokenize\n",
      "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
      "                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 119, in sent_tokenize\n",
      "    tokenizer = _get_punkt_tokenizer(language)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 105, in _get_punkt_tokenizer\n",
      "    return PunktTokenizer(language)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py\", line 1744, in __init__\n",
      "    self.load_lang(lang)\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py\", line 1749, in load_lang\n",
      "    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\ghali/nltk_data'\n",
      "    - 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\ghali\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\tkinter\\__init__.py\", line 1968, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ghali\\AppData\\Local\\Temp\\ipykernel_10836\\2050975476.py\", line 103, in corriger_interactivement\n",
      "    tokens = word_tokenize(phrase.lower(), language=\"english\" if lang == \"en\" else \"french\")\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 142, in word_tokenize\n",
      "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
      "                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 119, in sent_tokenize\n",
      "    tokenizer = _get_punkt_tokenizer(language)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 105, in _get_punkt_tokenizer\n",
      "    return PunktTokenizer(language)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py\", line 1744, in __init__\n",
      "    self.load_lang(lang)\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py\", line 1749, in load_lang\n",
      "    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\ghali/nltk_data'\n",
      "    - 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\ghali\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\tkinter\\__init__.py\", line 1968, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ghali\\AppData\\Local\\Temp\\ipykernel_10836\\2050975476.py\", line 103, in corriger_interactivement\n",
      "    tokens = word_tokenize(phrase.lower(), language=\"english\" if lang == \"en\" else \"french\")\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 142, in word_tokenize\n",
      "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
      "                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 119, in sent_tokenize\n",
      "    tokenizer = _get_punkt_tokenizer(language)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 105, in _get_punkt_tokenizer\n",
      "    return PunktTokenizer(language)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py\", line 1744, in __init__\n",
      "    self.load_lang(lang)\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py\", line 1749, in load_lang\n",
      "    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\ghali/nltk_data'\n",
      "    - 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\ghali\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\tkinter\\__init__.py\", line 1968, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ghali\\AppData\\Local\\Temp\\ipykernel_10836\\2050975476.py\", line 103, in corriger_interactivement\n",
      "    tokens = word_tokenize(phrase.lower(), language=\"english\" if lang == \"en\" else \"french\")\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 142, in word_tokenize\n",
      "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
      "                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 119, in sent_tokenize\n",
      "    tokenizer = _get_punkt_tokenizer(language)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 105, in _get_punkt_tokenizer\n",
      "    return PunktTokenizer(language)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py\", line 1744, in __init__\n",
      "    self.load_lang(lang)\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py\", line 1749, in load_lang\n",
      "    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\ghali/nltk_data'\n",
      "    - 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\ghali\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\tkinter\\__init__.py\", line 1968, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ghali\\AppData\\Local\\Temp\\ipykernel_10836\\2050975476.py\", line 103, in corriger_interactivement\n",
      "    tokens = word_tokenize(phrase.lower(), language=\"english\" if lang == \"en\" else \"french\")\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 142, in word_tokenize\n",
      "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
      "                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 119, in sent_tokenize\n",
      "    tokenizer = _get_punkt_tokenizer(language)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 105, in _get_punkt_tokenizer\n",
      "    return PunktTokenizer(language)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py\", line 1744, in __init__\n",
      "    self.load_lang(lang)\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py\", line 1749, in load_lang\n",
      "    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ghali\\anaconda3\\Lib\\site-packages\\nltk\\data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\ghali/nltk_data'\n",
      "    - 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\ghali\\\\anaconda3\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\ghali\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox\n",
    "import difflib\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import threading\n",
    "import nltk\n",
    "\n",
    "# Auto download if not found\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/gutenberg')\n",
    "except LookupError:\n",
    "    nltk.download('gutenberg')\n",
    "\n",
    "# Chargement du corpus\n",
    "def load_corpus(lang=\"en\"):\n",
    "    if lang == \"en\":\n",
    "        from nltk.corpus import gutenberg\n",
    "        texte = gutenberg.raw('austen-emma.txt')\n",
    "    elif lang == \"fr\":\n",
    "        with open(\"corpus_fr.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "            texte = f.read()\n",
    "    else:\n",
    "        raise ValueError(\"Langue non supportée.\")\n",
    "    return texte\n",
    "\n",
    "def build_vocabulary(texte, language=\"en\"):\n",
    "    lang = \"english\" if language == \"en\" else \"french\"\n",
    "    tokens = word_tokenize(texte.lower(), language=lang)\n",
    "    mots = [mot for mot in tokens if mot.isalpha()]\n",
    "    return set(mots), Counter(mots)\n",
    "\n",
    "def generer_candidats(mot, vocabulaire, n=3):\n",
    "    return difflib.get_close_matches(mot, vocabulaire, n=n)\n",
    "\n",
    "class AutoCorrectApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"🔤 Correcteur Interactif NLP\")\n",
    "        self.root.geometry(\"900x600\")\n",
    "\n",
    "        self.vocabulaire = {}\n",
    "        self.frequences = {}\n",
    "        self.suggestion_vars = []\n",
    "\n",
    "        self.build_gui()\n",
    "        self.auto_load_corpus(\"en\")\n",
    "\n",
    "    def build_gui(self):\n",
    "        top_frame = ttk.Frame(self.root)\n",
    "        top_frame.pack(pady=10)\n",
    "\n",
    "        ttk.Label(top_frame, text=\"Langue :\").pack(side=\"left\", padx=5)\n",
    "        self.language = tk.StringVar(value=\"en\")\n",
    "        self.language_menu = ttk.OptionMenu(top_frame, self.language, \"en\", \"en\", \"fr\", command=self.auto_load_corpus)\n",
    "        self.language_menu.pack(side=\"left\", padx=5)\n",
    "\n",
    "        self.status_label = ttk.Label(self.root, text=\"Chargement du corpus...\", foreground=\"blue\")\n",
    "        self.status_label.pack(pady=5)\n",
    "\n",
    "        self.text_input = tk.Text(self.root, width=90, height=7, font=(\"Arial\", 13))\n",
    "        self.text_input.pack(pady=10)\n",
    "\n",
    "        self.correct_button = ttk.Button(self.root, text=\"🔧 Proposer des corrections\", command=self.corriger_interactivement)\n",
    "        self.correct_button.pack(pady=5)\n",
    "\n",
    "        self.scroll_canvas = tk.Canvas(self.root)\n",
    "        self.scroll_frame = ttk.Frame(self.scroll_canvas)\n",
    "        self.scrollbar = ttk.Scrollbar(self.root, orient=\"vertical\", command=self.scroll_canvas.yview)\n",
    "        self.scroll_canvas.configure(yscrollcommand=self.scrollbar.set)\n",
    "\n",
    "        self.scroll_canvas.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "        self.scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "        self.canvas_frame = self.scroll_canvas.create_window((0, 0), window=self.scroll_frame, anchor='nw')\n",
    "        self.scroll_frame.bind(\"<Configure>\", lambda e: self.scroll_canvas.configure(scrollregion=self.scroll_canvas.bbox(\"all\")))\n",
    "\n",
    "        self.confirm_button = ttk.Button(self.root, text=\"✅ Confirmer les choix\", command=self.confirmer_correction)\n",
    "        self.confirm_button.pack(pady=10)\n",
    "\n",
    "        self.result_label = ttk.Label(self.root, text=\"\", wraplength=800, font=(\"Arial\", 12), foreground=\"green\")\n",
    "        self.result_label.pack(pady=5)\n",
    "\n",
    "    def auto_load_corpus(self, lang):\n",
    "        def run():\n",
    "            self.status_label.config(text=\"Chargement du corpus...\", foreground=\"orange\")\n",
    "            texte = load_corpus(lang)\n",
    "            self.vocabulaire[lang], self.frequences[lang] = build_vocabulary(texte, lang)\n",
    "            self.status_label.config(text=f\"✔ Corpus {lang.upper()} chargé ({len(self.vocabulaire[lang])} mots)\", foreground=\"green\")\n",
    "        threading.Thread(target=run).start()\n",
    "\n",
    "    def corriger_interactivement(self):\n",
    "        for widget in self.scroll_frame.winfo_children():\n",
    "            widget.destroy()\n",
    "        self.suggestion_vars.clear()\n",
    "\n",
    "        phrase = self.text_input.get(\"1.0\", \"end\").strip()\n",
    "        lang = self.language.get()\n",
    "        tokens = word_tokenize(phrase.lower(), language=\"english\" if lang == \"en\" else \"french\")\n",
    "\n",
    "        for mot in tokens:\n",
    "            frame = ttk.Frame(self.scroll_frame, padding=5)\n",
    "            frame.pack(fill=\"x\", pady=3)\n",
    "\n",
    "            ttk.Label(frame, text=f\"Mot : '{mot}'\").pack(side=\"left\", padx=5)\n",
    "            var = tk.StringVar(frame)\n",
    "\n",
    "            if mot.isalpha() and mot not in self.vocabulaire[lang]:\n",
    "                candidats = generer_candidats(mot, self.vocabulaire[lang], n=3)\n",
    "                if not candidats:\n",
    "                    candidats = [mot]\n",
    "                var.set(candidats[0])\n",
    "                option_menu = ttk.OptionMenu(frame, var, candidats[0], *candidats)\n",
    "                option_menu.pack(side=\"left\", padx=10)\n",
    "            else:\n",
    "                var.set(mot)\n",
    "                ttk.Label(frame, text=\"(Mot valide)\").pack(side=\"left\", padx=10)\n",
    "\n",
    "            self.suggestion_vars.append(var)\n",
    "\n",
    "    def confirmer_correction(self):\n",
    "        corrected = [var.get() for var in self.suggestion_vars]\n",
    "        phrase_corrigée = \" \".join(corrected)\n",
    "        self.result_label.config(text=f\"🟢 Texte corrigé :\\n{phrase_corrigée}\")\n",
    "        self.root.clipboard_clear()\n",
    "        self.root.clipboard_append(phrase_corrigée)\n",
    "        self.root.update()\n",
    "\n",
    "# Lancement\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = AutoCorrectApp(root)\n",
    "    root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
